{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "resnet18-cifar100.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/panda1230/Adversarial_NoiseLearning_NoL/blob/master/resnet18_cifar100.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ua3fJ9VdmLnl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e61cb406-7b8c-4bbd-f042-64c50e8592b2"
      },
      "source": [
        "'''Initialize the network architecture'''\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch.backends.cudnn as cudnn\n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "size_1 = 64   # 32 x 32\n",
        "size_2 = 64\n",
        "size_3 = 64\n",
        "size_4 = 64\n",
        "size_5 = 64\n",
        "size_6 = 128  # 16 x 16\n",
        "size_7 = 128\n",
        "size_8 = 128\n",
        "size_9 = 128\n",
        "size_10 = 256 # 8 x 8\n",
        "size_11 = 256\n",
        "size_12 = 256\n",
        "size_13 = 256\n",
        "size_14 = 512 # 4 x 4\n",
        "size_15 = 512\n",
        "size_16 = 512\n",
        "size_17 = 512\n",
        "\n",
        "\n",
        "class Net(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, size_1, kernel_size = 3, stride = 1, padding = 1, bias = False)\n",
        "        self.norm1 = nn.BatchNorm2d(size_1)\n",
        "\n",
        "        # BLOCK 1 #\n",
        "        self.conv2 = nn.Conv2d(size_1, size_2, kernel_size = 3, stride = 1, padding = 1, bias = False)\n",
        "        self.norm2 = nn.BatchNorm2d(size_2)\n",
        "        self.conv3 = nn.Conv2d(size_2, size_3, kernel_size = 3, stride = 1, padding = 1, bias = False)\n",
        "        self.norm3 = nn.BatchNorm2d(size_3)\n",
        "        self.shortcut1 = nn.Conv2d(size_1, size_3, kernel_size = 1, stride = 1, padding = 0, bias = False)\n",
        "        self.normS1 = nn.BatchNorm2d(size_3)\n",
        "\n",
        "        self.conv4 = nn.Conv2d(size_3, size_4, kernel_size = 3, stride = 1, padding = 1, bias = False)\n",
        "        self.norm4 = nn.BatchNorm2d(size_4)\n",
        "        self.conv5 = nn.Conv2d(size_4, size_5, kernel_size = 3, stride = 1, padding = 1, bias = False)\n",
        "        self.norm5 = nn.BatchNorm2d(size_5)\n",
        "        self.shortcut2 = nn.Conv2d(size_3, size_5, kernel_size = 1, stride = 1, padding = 0, bias = False)\n",
        "        self.normS2 = nn.BatchNorm2d(size_5)\n",
        "\n",
        "\n",
        "        # BLOCK 2 #\n",
        "        self.conv6 = nn.Conv2d(size_5, size_6, kernel_size = 3, stride = 2, padding = 1, bias = False)\n",
        "        self.norm6 = nn.BatchNorm2d(size_6)\n",
        "        self.conv7 = nn.Conv2d(size_6, size_7, kernel_size = 3, stride = 1, padding = 1, bias = False)\n",
        "        self.norm7 = nn.BatchNorm2d(size_7)\n",
        "        self.shortcut3 = nn.Conv2d(size_5, size_7, kernel_size = 1, stride = 2, padding = 0, bias = False)\n",
        "        self.normS3 = nn.BatchNorm2d(size_7)\n",
        "        \n",
        "        self.conv8 = nn.Conv2d(size_7, size_8, kernel_size = 3, stride = 1, padding = 1, bias = False)\n",
        "        self.norm8 = nn.BatchNorm2d(size_8)\n",
        "        self.conv9 = nn.Conv2d(size_8, size_9, kernel_size = 3, stride = 1, padding = 1, bias = False)\n",
        "        self.norm9 = nn.BatchNorm2d(size_9)\n",
        "        self.shortcut4 = nn.Conv2d(size_7, size_9, kernel_size = 1, stride = 1, padding = 0, bias = False)\n",
        "        self.normS4 = nn.BatchNorm2d(size_9)\n",
        "\n",
        "\n",
        "        # BLOCK 3 #\n",
        "        self.conv10 = nn.Conv2d(size_9, size_10, kernel_size = 3, stride = 2, padding = 1, bias = False)\n",
        "        self.norm10 = nn.BatchNorm2d(size_10)\n",
        "        self.conv11 = nn.Conv2d(size_10, size_11, kernel_size = 3, stride = 1, padding = 1, bias = False)\n",
        "        self.norm11 = nn.BatchNorm2d(size_11)\n",
        "        self.shortcut5 = nn.Conv2d(size_9, size_11, kernel_size = 1, stride = 2, padding = 0, bias = False)\n",
        "        self.normS5 = nn.BatchNorm2d(size_11)\n",
        "        \n",
        "        self.conv12 = nn.Conv2d(size_11, size_12, kernel_size = 3, stride = 1, padding = 1, bias = False)\n",
        "        self.norm12 = nn.BatchNorm2d(size_12)\n",
        "        self.conv13 = nn.Conv2d(size_12, size_13, kernel_size = 3, stride = 1, padding = 1, bias = False)\n",
        "        self.norm13 = nn.BatchNorm2d(size_13)\n",
        "        self.shortcut6 = nn.Conv2d(size_11, size_13, kernel_size = 1, stride = 1, padding = 0, bias = False)\n",
        "        self.normS6 = nn.BatchNorm2d(size_13)\n",
        "\n",
        "\n",
        "        # BLOCK 4 #\n",
        "        self.conv14 = nn.Conv2d(size_13, size_14, kernel_size = 3, stride = 2, padding = 1, bias = False)\n",
        "        self.norm14 = nn.BatchNorm2d(size_14)\n",
        "        self.conv15 = nn.Conv2d(size_14, size_15, kernel_size = 3, stride = 1, padding = 1, bias = False)\n",
        "        self.norm15 = nn.BatchNorm2d(size_15)\n",
        "        self.shortcut7 = nn.Conv2d(size_13, size_15, kernel_size = 1, stride = 2, padding = 0, bias = False)\n",
        "        self.normS7 = nn.BatchNorm2d(size_15)\n",
        "        \n",
        "        self.conv16 = nn.Conv2d(size_15, size_16, kernel_size = 3, stride = 1, padding = 1, bias = False)\n",
        "        self.norm16 = nn.BatchNorm2d(size_16)\n",
        "        self.conv17 = nn.Conv2d(size_16, size_17, kernel_size = 3, stride = 1, padding = 1, bias = False)\n",
        "        self.norm17 = nn.BatchNorm2d(size_17)\n",
        "        self.shortcut8 = nn.Conv2d(size_15, size_17, kernel_size = 1, stride = 1, padding = 0, bias = False)\n",
        "        self.normS8 = nn.BatchNorm2d(size_17)\n",
        "\n",
        "        self.linear = nn.Linear(512, 100)\n",
        "\n",
        "\n",
        "    def forward(self, x0):\n",
        "        x1 = F.relu(self.norm1(self.conv1(x0)))        # x1 has size 64 (i.e. it has 64 filters)\n",
        "\n",
        "        # BLOCK 1 #\n",
        "        x2 = F.relu(self.norm2(self.conv2(x1)))         # x2 has size 64\n",
        "        x3 = F.relu(self.norm3(self.conv3(x2)))         # x3 has size 64\n",
        "        xS1 = F.relu(self.normS1(self.shortcut1(x1)))   # have to project x1 to have the same size as x3\n",
        "        x3 = x3 + xS1                                   \n",
        "        x4 = F.relu(self.norm4(self.conv4(x3)))         # x4 has size 64\n",
        "        x5 = F.relu(self.norm5(self.conv5(x4)))         # x5 has size 64\n",
        "        xS2 = F.relu(self.normS2(self.shortcut2(x3)))   # have to project x3 to have the same size as x5\n",
        "        x5 = x5 + xS2\n",
        "        \n",
        "\n",
        "        # BLOCK 2 #\n",
        "        x6 = F.relu(self.norm6(self.conv6(x5)))         # x6 has size 128\n",
        "        x7 = F.relu(self.norm7(self.conv7(x6)))         # x7 has size 128\n",
        "        xS3 = F.relu(self.normS3(self.shortcut3(x5)))   # have to project x5 to have the same size as x7\n",
        "        x7 = x7 + xS3\n",
        "        x8 = F.relu(self.norm8(self.conv8(x7)))         # x8 has size 128\n",
        "        x9 = F.relu(self.norm9(self.conv9(x8)))         # x9 has size 128\n",
        "        xS4 = F.relu(self.normS4(self.shortcut4(x7)))   # have to project x7 to have the same size as x9\n",
        "        x9 = x9 + xS4\n",
        "\n",
        "        # BLOCK 3 #\n",
        "        x10 = F.relu(self.norm10(self.conv10(x9)))      # x10 has size 256\n",
        "        x11 = F.relu(self.norm11(self.conv11(x10)))     # x11 has size 256\n",
        "        xS5 = F.relu(self.normS5(self.shortcut5(x9)))   # have to project x9 to have the same size as x11\n",
        "        x11 = x11 + xS5\n",
        "        x12 = F.relu(self.norm12(self.conv12(x11)))     # x12 has size 256\n",
        "        x13 = F.relu(self.norm13(self.conv13(x12)))     # x13 has size 256\n",
        "        xS6 = F.relu(self.normS6(self.shortcut6(x11)))  # have to project x11 to have the same size as x13\n",
        "        x13 = x13 + xS6\n",
        "\n",
        "        # BLOCK 4 #\n",
        "        x14 = F.relu(self.norm14(self.conv14(x13)))     # x14 has size 512\n",
        "        x15 = F.relu(self.norm15(self.conv15(x14)))     # x15 has size 512\n",
        "        xS7 = F.relu(self.normS7(self.shortcut7(x13)))  # have to project x13 to have the same size as x15\n",
        "        x15 = x15 + xS7\n",
        "        x16 = F.relu(self.norm16(self.conv16(x15)))     # x16 has size 512\n",
        "        x17 = F.relu(self.norm17(self.conv17(x16)))     # x17 has size 512\n",
        "        xS8 = F.relu(self.normS8(self.shortcut8(x15)))  # have to project x15 to have the same size as x17\n",
        "        x17 = x17 + xS8\n",
        "\n",
        "        x18 = F.avg_pool2d(x17, 4)\n",
        "        x18 = x18.view(x18.size(0), -1)\n",
        "        x19 = self.linear(x18)\n",
        "\n",
        "        output = x19\n",
        "        activations = [x1, x2, x3, x4, x5, x6, x7, x8, x9, x10, x11, x12, x13, x14, x15, x16, x17]\n",
        "\n",
        "        return output, activations\n",
        "\n",
        "\n",
        "def test():\n",
        "    net = Net()\n",
        "    #net.eval()\n",
        "    y, x = net(torch.randn(1,3,32,32))\n",
        "    print(y.size())\n",
        "\n",
        "test()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([1, 100])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O0_df8U8mRd6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ded7e3b2-50b1-43a6-c082-da1580395b45"
      },
      "source": [
        "'''train network'''\n",
        "\n",
        "device = 'cuda'\n",
        "best_acc = 0  # best test accuracy\n",
        "start_epoch = 0  # start from epoch 0 or last checkpoint epoch\n",
        "num_epochs = 210\n",
        "num_layers = 17\n",
        "\n",
        "absolute_layer_energies = np.zeros((num_epochs, num_layers+1))\n",
        "fractional_layer_energies = np.zeros((num_epochs, num_layers+1))\n",
        "\n",
        "# Data\n",
        "print('==> Preparing data..')\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR100(root='./../datasets/cifar100', train=True, download=True, transform=transform_train)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=256, shuffle=True, num_workers=2)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR100(root='./../datasets/cifar100', train=False, download=True, transform=transform_test)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=100, shuffle=False, num_workers=2)\n",
        "\n",
        "#classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "\n",
        "print('==> Building model..')\n",
        "net = Net()\n",
        "net = net.to(device)\n",
        "net = torch.nn.DataParallel(net)\n",
        "cudnn.benchmark = True\n",
        "\n",
        "test_acc = []\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)\n",
        "\n",
        "# Training\n",
        "def train(epoch):\n",
        "    print('\\nEpoch: %d' % epoch)\n",
        "    net.train()\n",
        "    train_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    activations = []\n",
        "    global absolute_layer_energies\n",
        "    global fractional_layer_energies\n",
        "    previous_time = time.process_time()\n",
        "    \n",
        "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs, activations = net(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += targets.size(0)\n",
        "        correct += predicted.eq(targets).sum().item()\n",
        "        if batch_idx%100==0:\n",
        "            current_time = time.process_time()\n",
        "            print(batch_idx, len(testloader), 'Lap time (s): %.2f | Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
        "                    % (current_time - previous_time, train_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
        "            previous_time = current_time\n",
        "            \n",
        "    this_epoch_abs_energies, this_epoch_frac_energies = count_non_zeros(activations)\n",
        "    fractional_layer_energies[epoch] = this_epoch_frac_energies\n",
        "    absolute_layer_energies[epoch] = this_epoch_abs_energies\n",
        "    print('Total activation density: %.3f' % (fractional_layer_energies[epoch, 0]))\n",
        "\n",
        "\n",
        "def test(epoch):\n",
        "    global best_acc\n",
        "    net.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    activations = []\n",
        "    previous_time = time.process_time()\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs, activations = net(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            test_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += targets.size(0)\n",
        "            correct += predicted.eq(targets).sum().item()\n",
        "            acc = correct/total\n",
        "            if batch_idx%100 == 0:\n",
        "                test_acc.append(acc)\n",
        "                best_acc = max(acc, best_acc)\n",
        "                current_time = time.process_time()\n",
        "                print(batch_idx, len(testloader), 'Lap time (s): %.2f | Loss: %.3f | Acc: %.3f%% (%d/%d) | Best acc: %.3f'\n",
        "                    % (current_time - previous_time, test_loss/(batch_idx+1), 100.*correct/total, correct, total, best_acc))\n",
        "                previous_time = current_time\n",
        "            if acc>=best_acc:\n",
        "                #print('Saving')\n",
        "                torch.save(net.state_dict(),'./sample_data/resnet18_net0.pth')\n",
        "\n",
        "\n",
        "def count_non_zeros(activations): \n",
        "    \n",
        "    #returns: numpy array containing the number of non-zero activations per layer (15x1)\n",
        "    #         numpy array containing the fraction of non-zero activations per layer (15x1)\n",
        "    \n",
        "    n = 0\n",
        "    num_zeros = np.zeros((num_layers+1,), dtype = int)\n",
        "    num_non_zeros = np.zeros((num_layers+1,), dtype = int)\n",
        "    total_activations = np.zeros((num_layers+1,), dtype = int)\n",
        "    fraction_non_zero = np.zeros((num_layers+1,), dtype = float)\n",
        "    for x in activations:\n",
        "        n += 1\n",
        "        #reshape activations into a flat list\n",
        "        num_activations = x.size()[0] * x.size()[1] * x.size()[2] * x.size()[3]\n",
        "        \n",
        "        y = x.view(num_activations).tolist()\n",
        "        \n",
        "        #count how many entries are zero / non-zero\n",
        "        num_zeros[n] = y.count(0)\n",
        "        total_activations[n] = num_activations\n",
        "        num_non_zeros[n] = len(y) - num_zeros[n]\n",
        "        fraction_non_zero[n] = num_non_zeros[n].astype(float)/float(len(y))\n",
        "     \n",
        "        \n",
        "    #store total values in the zero slot\n",
        "    num_non_zeros[0] = np.sum(num_non_zeros) \n",
        "    total_activations[0] = np.sum(total_activations)\n",
        "    fraction_non_zero[0] = num_non_zeros[0].astype(float)/total_activations[0].astype(float)\n",
        "    return num_non_zeros, fraction_non_zero\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    if (epoch == 70):\n",
        "        optimizer = optim.SGD(net.parameters(), lr=0.01, momentum=0.9, weight_decay=5e-4)\n",
        "    if (epoch == 140):\n",
        "        optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9, weight_decay=5e-4)\n",
        "    train(epoch)\n",
        "    test(epoch)\n",
        "    print('Elapsed time: %.2f' % (time.process_time()))\n",
        "    \n",
        "torch.save(fractional_layer_energies,'./sample_data/fraction_energy_net0.pth')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "==> Preparing data..\n",
            "Using downloaded and verified file: ./../datasets/cifar100/cifar-100-python.tar.gz\n",
            "Extracting ./../datasets/cifar100/cifar-100-python.tar.gz to ./../datasets/cifar100\n",
            "Files already downloaded and verified\n",
            "==> Building model..\n",
            "\n",
            "Epoch: 0\n",
            "0 100 Lap time (s): 0.52 | Loss: 4.694 | Acc: 1.562% (4/256)\n",
            "100 100 Lap time (s): 11.60 | Loss: 4.405 | Acc: 4.115% (1064/25856)\n",
            "Total activation density: 0.598\n",
            "0 100 Lap time (s): 0.16 | Loss: 3.960 | Acc: 7.000% (7/100) | Best acc: 0.070\n",
            "Elapsed time: 47.05\n",
            "\n",
            "Epoch: 1\n",
            "0 100 Lap time (s): 0.15 | Loss: 3.791 | Acc: 10.938% (28/256)\n",
            "100 100 Lap time (s): 11.56 | Loss: 3.688 | Acc: 11.881% (3072/25856)\n",
            "Total activation density: 0.596\n",
            "0 100 Lap time (s): 0.08 | Loss: 3.770 | Acc: 9.000% (9/100) | Best acc: 0.090\n",
            "Elapsed time: 82.33\n",
            "\n",
            "Epoch: 2\n",
            "0 100 Lap time (s): 0.14 | Loss: 3.462 | Acc: 20.312% (52/256)\n",
            "100 100 Lap time (s): 11.55 | Loss: 3.261 | Acc: 19.554% (5056/25856)\n",
            "Total activation density: 0.591\n",
            "0 100 Lap time (s): 0.07 | Loss: 3.248 | Acc: 27.000% (27/100) | Best acc: 0.270\n",
            "Elapsed time: 109.42\n",
            "\n",
            "Epoch: 3\n",
            "0 100 Lap time (s): 0.15 | Loss: 2.819 | Acc: 25.000% (64/256)\n",
            "100 100 Lap time (s): 11.57 | Loss: 2.891 | Acc: 26.508% (6854/25856)\n",
            "Total activation density: 0.587\n",
            "0 100 Lap time (s): 0.08 | Loss: 2.843 | Acc: 28.000% (28/100) | Best acc: 0.280\n",
            "Elapsed time: 144.86\n",
            "\n",
            "Epoch: 4\n",
            "0 100 Lap time (s): 0.18 | Loss: 2.548 | Acc: 33.203% (85/256)\n",
            "100 100 Lap time (s): 11.60 | Loss: 2.496 | Acc: 34.216% (8847/25856)\n",
            "Total activation density: 0.584\n",
            "0 100 Lap time (s): 0.08 | Loss: 2.628 | Acc: 28.000% (28/100) | Best acc: 0.280\n",
            "Elapsed time: 180.40\n",
            "\n",
            "Epoch: 5\n",
            "0 100 Lap time (s): 0.15 | Loss: 2.136 | Acc: 42.188% (108/256)\n",
            "100 100 Lap time (s): 11.54 | Loss: 2.148 | Acc: 41.619% (10761/25856)\n",
            "Total activation density: 0.579\n",
            "0 100 Lap time (s): 0.08 | Loss: 2.410 | Acc: 42.000% (42/100) | Best acc: 0.420\n",
            "Elapsed time: 207.52\n",
            "\n",
            "Epoch: 6\n",
            "0 100 Lap time (s): 0.13 | Loss: 1.913 | Acc: 47.266% (121/256)\n",
            "100 100 Lap time (s): 11.63 | Loss: 1.911 | Acc: 46.890% (12124/25856)\n",
            "Total activation density: 0.573\n",
            "0 100 Lap time (s): 0.08 | Loss: 1.943 | Acc: 51.000% (51/100) | Best acc: 0.510\n",
            "Elapsed time: 234.72\n",
            "\n",
            "Epoch: 7\n",
            "0 100 Lap time (s): 0.14 | Loss: 1.708 | Acc: 50.781% (130/256)\n",
            "100 100 Lap time (s): 11.62 | Loss: 1.707 | Acc: 51.887% (13416/25856)\n",
            "Total activation density: 0.568\n",
            "0 100 Lap time (s): 0.08 | Loss: 2.244 | Acc: 41.000% (41/100) | Best acc: 0.510\n",
            "Elapsed time: 261.72\n",
            "\n",
            "Epoch: 8\n",
            "0 100 Lap time (s): 0.15 | Loss: 1.576 | Acc: 54.297% (139/256)\n",
            "100 100 Lap time (s): 11.62 | Loss: 1.581 | Acc: 55.202% (14273/25856)\n",
            "Total activation density: 0.565\n",
            "0 100 Lap time (s): 0.07 | Loss: 1.817 | Acc: 56.000% (56/100) | Best acc: 0.560\n",
            "Elapsed time: 288.82\n",
            "\n",
            "Epoch: 9\n",
            "0 100 Lap time (s): 0.16 | Loss: 1.468 | Acc: 57.031% (146/256)\n",
            "100 100 Lap time (s): 11.60 | Loss: 1.459 | Acc: 57.948% (14983/25856)\n",
            "Total activation density: 0.560\n",
            "0 100 Lap time (s): 0.08 | Loss: 1.921 | Acc: 53.000% (53/100) | Best acc: 0.560\n",
            "Elapsed time: 315.80\n",
            "\n",
            "Epoch: 10\n",
            "0 100 Lap time (s): 0.14 | Loss: 1.340 | Acc: 60.938% (156/256)\n",
            "100 100 Lap time (s): 11.58 | Loss: 1.384 | Acc: 60.272% (15584/25856)\n",
            "Total activation density: 0.557\n",
            "0 100 Lap time (s): 0.08 | Loss: 1.576 | Acc: 60.000% (60/100) | Best acc: 0.600\n",
            "Elapsed time: 342.81\n",
            "\n",
            "Epoch: 11\n",
            "0 100 Lap time (s): 0.16 | Loss: 1.304 | Acc: 64.844% (166/256)\n",
            "100 100 Lap time (s): 11.60 | Loss: 1.292 | Acc: 62.450% (16147/25856)\n",
            "Total activation density: 0.552\n",
            "0 100 Lap time (s): 0.08 | Loss: 1.620 | Acc: 54.000% (54/100) | Best acc: 0.600\n",
            "Elapsed time: 369.80\n",
            "\n",
            "Epoch: 12\n",
            "0 100 Lap time (s): 0.15 | Loss: 1.103 | Acc: 66.797% (171/256)\n",
            "100 100 Lap time (s): 11.64 | Loss: 1.228 | Acc: 64.287% (16622/25856)\n",
            "Total activation density: 0.551\n",
            "0 100 Lap time (s): 0.08 | Loss: 1.781 | Acc: 57.000% (57/100) | Best acc: 0.600\n",
            "Elapsed time: 396.77\n",
            "\n",
            "Epoch: 13\n",
            "0 100 Lap time (s): 0.15 | Loss: 1.161 | Acc: 66.797% (171/256)\n",
            "100 100 Lap time (s): 11.59 | Loss: 1.169 | Acc: 65.737% (16997/25856)\n",
            "Total activation density: 0.549\n",
            "0 100 Lap time (s): 0.07 | Loss: 1.831 | Acc: 53.000% (53/100) | Best acc: 0.600\n",
            "Elapsed time: 423.83\n",
            "\n",
            "Epoch: 14\n",
            "0 100 Lap time (s): 0.15 | Loss: 1.051 | Acc: 69.531% (178/256)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V6YqVQInmYyY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "total_energy = fractional_layer_energies[:,0]\n",
        "conv1_energy = fractional_layer_energies[:,1]\n",
        "conv2_energy = fractional_layer_energies[:,2]\n",
        "conv3_energy = fractional_layer_energies[:,3]\n",
        "conv4_energy = fractional_layer_energies[:,4]\n",
        "conv5_energy = fractional_layer_energies[:,5]\n",
        "conv6_energy = fractional_layer_energies[:,6]\n",
        "conv7_energy = fractional_layer_energies[:,7]\n",
        "conv8_energy = fractional_layer_energies[:,8]\n",
        "conv9_energy = fractional_layer_energies[:,9]\n",
        "conv10_energy = fractional_layer_energies[:,10]\n",
        "conv11_energy = fractional_layer_energies[:,11]\n",
        "conv12_energy = fractional_layer_energies[:,12]\n",
        "conv13_energy = fractional_layer_energies[:,13]\n",
        "conv14_energy = fractional_layer_energies[:,14]\n",
        "conv15_energy = fractional_layer_energies[:,15]\n",
        "conv16_energy = fractional_layer_energies[:,16]\n",
        "conv17_energy = fractional_layer_energies[:,17]\n",
        "\n",
        "\n",
        "\n",
        "ylim = 1\n",
        "ylow = 0 \n",
        "\n",
        "fig = plt.figure(1, figsize=(15, 7), dpi=95)\n",
        "plt.subplot(231)\n",
        "plt.ylim(0, 1)\n",
        "plt.plot(test_acc, label = 'Test accuracy')\n",
        "plt.legend(frameon = False)\n",
        "\n",
        "plt.subplot(232)\n",
        "plt.ylim(ylow, ylim)\n",
        "plt.plot(total_energy, label = 'Total energy')\n",
        "plt.legend(frameon = False)\n",
        "\n",
        "plt.subplot(233)\n",
        "plt.ylim(ylow, ylim)\n",
        "#plt.plot(conv1_energy, label = 'Conv1 energy')\n",
        "plt.plot(conv2_energy, label = 'Conv2 energy')\n",
        "plt.plot(conv3_energy, label = 'Conv3 energy')\n",
        "plt.plot(conv4_energy, label = 'Conv4 energy')\n",
        "plt.plot(conv5_energy, label = 'Conv5 energy')\n",
        "plt.legend(frameon = False)\n",
        "\n",
        "plt.subplot(234)\n",
        "plt.ylim(ylow, ylim)\n",
        "plt.plot(conv6_energy, label = 'Conv6 energy')\n",
        "plt.plot(conv7_energy, label = 'Conv7 energy')\n",
        "plt.plot(conv8_energy, label = 'Conv8 energy')\n",
        "plt.plot(conv9_energy, label = 'Conv9 energy')\n",
        "plt.legend(frameon = False)\n",
        "\n",
        "plt.subplot(235)\n",
        "plt.ylim(ylow, ylim)\n",
        "plt.plot(conv10_energy, label = 'Conv10 energy')\n",
        "plt.plot(conv11_energy, label = 'Conv11 energy')\n",
        "plt.plot(conv12_energy, label = 'Conv12 energy')\n",
        "plt.plot(conv13_energy, label = 'Conv13 energy')\n",
        "plt.legend(frameon = False)\n",
        "\n",
        "plt.subplot(236)\n",
        "plt.ylim(ylow, ylim)\n",
        "plt.plot(conv14_energy, label = 'Conv14 energy')\n",
        "plt.plot(conv15_energy, label = 'Conv15 energy')\n",
        "plt.plot(conv16_energy, label = 'Conv16 energy')\n",
        "plt.plot(conv17_energy, label = 'Conv17 energy')\n",
        "plt.legend(frameon = False)\n",
        "\n",
        "\n",
        "fig.suptitle('Fractional activation energy (sparsity) vs. epoch', fontsize = 16)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z4kGE624ncW5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_epochs -1 =50\n",
        "\n",
        "size_1 = int(fractional_layer_energies[num_epochs-1, 1] * 64)\n",
        "size_2 = int(fractional_layer_energies[num_epochs-1, 2] * 64)\n",
        "size_3 = int(fractional_layer_energies[num_epochs-1, 3] * 64)\n",
        "size_4 = int(fractional_layer_energies[num_epochs-1, 4] * 64)\n",
        "size_5 = int(fractional_layer_energies[num_epochs-1, 5] * 64)\n",
        "size_6 = int(fractional_layer_energies[num_epochs-1, 6] * 128)\n",
        "size_7 = int(fractional_layer_energies[num_epochs-1, 7] * 128)\n",
        "size_8 = int(fractional_layer_energies[num_epochs-1, 8] * 128)\n",
        "size_9 = int(fractional_layer_energies[num_epochs-1, 9] * 128)\n",
        "size_10 = int(fractional_layer_energies[num_epochs-1, 10] * 256)\n",
        "size_11 = int(fractional_layer_energies[num_epochs-1, 11] * 256)\n",
        "size_12 = int(fractional_layer_energies[num_epochs-1, 12] * 256)\n",
        "size_13 = int(fractional_layer_energies[num_epochs-1, 13] * 256)\n",
        "size_14 = int(fractional_layer_energies[num_epochs-1, 14] * 512)\n",
        "size_15 = int(fractional_layer_energies[num_epochs-1, 15] * 512)\n",
        "size_16 = int(fractional_layer_energies[num_epochs-1, 16] * 512)\n",
        "size_17 = int(fractional_layer_energies[num_epochs-1, 17] * 512)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GBlgvcVtnz2H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Net1(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self):\n",
        "        super(Net1, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, size_1, kernel_size = 3, stride = 1, padding = 1, bias = False)\n",
        "        self.norm1 = nn.BatchNorm2d(size_1)\n",
        "\n",
        "        # BLOCK 1 #\n",
        "        self.conv2 = nn.Conv2d(size_1, size_2, kernel_size = 3, stride = 1, padding = 1, bias = False)\n",
        "        self.norm2 = nn.BatchNorm2d(size_2)\n",
        "        self.conv3 = nn.Conv2d(size_2, size_3, kernel_size = 3, stride = 1, padding = 1, bias = False)\n",
        "        self.norm3 = nn.BatchNorm2d(size_3)\n",
        "        self.shortcut1 = nn.Conv2d(size_1, size_3, kernel_size = 1, stride = 1, padding = 0, bias = False)\n",
        "        self.normS1 = nn.BatchNorm2d(size_3)\n",
        "\n",
        "        self.conv4 = nn.Conv2d(size_3, size_4, kernel_size = 3, stride = 1, padding = 1, bias = False)\n",
        "        self.norm4 = nn.BatchNorm2d(size_4)\n",
        "        self.conv5 = nn.Conv2d(size_4, size_5, kernel_size = 3, stride = 1, padding = 1, bias = False)\n",
        "        self.norm5 = nn.BatchNorm2d(size_5)\n",
        "        self.shortcut2 = nn.Conv2d(size_3, size_5, kernel_size = 1, stride = 1, padding = 0, bias = False)\n",
        "        self.normS2 = nn.BatchNorm2d(size_5)\n",
        "\n",
        "\n",
        "        # BLOCK 2 #\n",
        "        self.conv6 = nn.Conv2d(size_5, size_6, kernel_size = 3, stride = 2, padding = 1, bias = False)\n",
        "        self.norm6 = nn.BatchNorm2d(size_6)\n",
        "        self.conv7 = nn.Conv2d(size_6, size_7, kernel_size = 3, stride = 1, padding = 1, bias = False)\n",
        "        self.norm7 = nn.BatchNorm2d(size_7)\n",
        "        self.shortcut3 = nn.Conv2d(size_5, size_7, kernel_size = 1, stride = 2, padding = 0, bias = False)\n",
        "        self.normS3 = nn.BatchNorm2d(size_7)\n",
        "        \n",
        "        self.conv8 = nn.Conv2d(size_7, size_8, kernel_size = 3, stride = 1, padding = 1, bias = False)\n",
        "        self.norm8 = nn.BatchNorm2d(size_8)\n",
        "        self.conv9 = nn.Conv2d(size_8, size_9, kernel_size = 3, stride = 1, padding = 1, bias = False)\n",
        "        self.norm9 = nn.BatchNorm2d(size_9)\n",
        "        self.shortcut4 = nn.Conv2d(size_7, size_9, kernel_size = 1, stride = 1, padding = 0, bias = False)\n",
        "        self.normS4 = nn.BatchNorm2d(size_9)\n",
        "\n",
        "\n",
        "        # BLOCK 3 #\n",
        "        self.conv10 = nn.Conv2d(size_9, size_10, kernel_size = 3, stride = 2, padding = 1, bias = False)\n",
        "        self.norm10 = nn.BatchNorm2d(size_10)\n",
        "        self.conv11 = nn.Conv2d(size_10, size_11, kernel_size = 3, stride = 1, padding = 1, bias = False)\n",
        "        self.norm11 = nn.BatchNorm2d(size_11)\n",
        "        self.shortcut5 = nn.Conv2d(size_9, size_11, kernel_size = 1, stride = 2, padding = 0, bias = False)\n",
        "        self.normS5 = nn.BatchNorm2d(size_11)\n",
        "        \n",
        "        self.conv12 = nn.Conv2d(size_11, size_12, kernel_size = 3, stride = 1, padding = 1, bias = False)\n",
        "        self.norm12 = nn.BatchNorm2d(size_12)\n",
        "        self.conv13 = nn.Conv2d(size_12, size_13, kernel_size = 3, stride = 1, padding = 1, bias = False)\n",
        "        self.norm13 = nn.BatchNorm2d(size_13)\n",
        "        self.shortcut6 = nn.Conv2d(size_11, size_13, kernel_size = 1, stride = 1, padding = 0, bias = False)\n",
        "        self.normS6 = nn.BatchNorm2d(size_13)\n",
        "\n",
        "\n",
        "        # BLOCK 4 #\n",
        "        self.conv14 = nn.Conv2d(size_13, size_14, kernel_size = 3, stride = 2, padding = 1, bias = False)\n",
        "        self.norm14 = nn.BatchNorm2d(size_14)\n",
        "        self.conv15 = nn.Conv2d(size_14, size_15, kernel_size = 3, stride = 1, padding = 1, bias = False)\n",
        "        self.norm15 = nn.BatchNorm2d(size_15)\n",
        "        self.shortcut7 = nn.Conv2d(size_13, size_15, kernel_size = 1, stride = 2, padding = 0, bias = False)\n",
        "        self.normS7 = nn.BatchNorm2d(size_15)\n",
        "        \n",
        "        self.conv16 = nn.Conv2d(size_15, size_16, kernel_size = 3, stride = 1, padding = 1, bias = False)\n",
        "        self.norm16 = nn.BatchNorm2d(size_16)\n",
        "        self.conv17 = nn.Conv2d(size_16, size_17, kernel_size = 3, stride = 1, padding = 1, bias = False)\n",
        "        self.norm17 = nn.BatchNorm2d(size_17)\n",
        "        self.shortcut8 = nn.Conv2d(size_15, size_17, kernel_size = 1, stride = 1, padding = 0, bias = False)\n",
        "        self.normS8 = nn.BatchNorm2d(size_17)\n",
        "\n",
        "        self.linear = nn.Linear(size_17, 10)\n",
        "\n",
        "\n",
        "    def forward(self, x0):\n",
        "        x1 = F.relu(self.norm1(self.conv1(x0)))        # x1 has size 64 (i.e. it has 64 filters)\n",
        "\n",
        "        # BLOCK 1 #\n",
        "        x2 = F.relu(self.norm2(self.conv2(x1)))         # x2 has size 64\n",
        "        x3 = F.relu(self.norm3(self.conv3(x2)))         # x3 has size 64\n",
        "        xS1 = F.relu(self.normS1(self.shortcut1(x1)))   # have to project x1 to have the same size as x3\n",
        "        x3 = x3 + xS1                                   \n",
        "        x4 = F.relu(self.norm4(self.conv4(x3)))         # x4 has size 64\n",
        "        x5 = F.relu(self.norm5(self.conv5(x4)))         # x5 has size 64\n",
        "        xS2 = F.relu(self.normS2(self.shortcut2(x3)))   # have to project x3 to have the same size as x5\n",
        "        x5 = x5 + xS2\n",
        "        \n",
        "\n",
        "        # BLOCK 2 #\n",
        "        x6 = F.relu(self.norm6(self.conv6(x5)))         # x6 has size 128\n",
        "        x7 = F.relu(self.norm7(self.conv7(x6)))         # x7 has size 128\n",
        "        xS3 = F.relu(self.normS3(self.shortcut3(x5)))   # have to project x5 to have the same size as x7\n",
        "        x7 = x7 + xS3\n",
        "        x8 = F.relu(self.norm8(self.conv8(x7)))         # x8 has size 128\n",
        "        x9 = F.relu(self.norm9(self.conv9(x8)))         # x9 has size 128\n",
        "        xS4 = F.relu(self.normS4(self.shortcut4(x7)))   # have to project x7 to have the same size as x9\n",
        "        x9 = x9 + xS4\n",
        "\n",
        "        # BLOCK 3 #\n",
        "        x10 = F.relu(self.norm10(self.conv10(x9)))      # x10 has size 256\n",
        "        x11 = F.relu(self.norm11(self.conv11(x10)))     # x11 has size 256\n",
        "        xS5 = F.relu(self.normS5(self.shortcut5(x9)))   # have to project x9 to have the same size as x11\n",
        "        x11 = x11 + xS5\n",
        "        x12 = F.relu(self.norm12(self.conv12(x11)))     # x12 has size 256\n",
        "        x13 = F.relu(self.norm13(self.conv13(x12)))     # x13 has size 256\n",
        "        xS6 = F.relu(self.normS6(self.shortcut6(x11)))  # have to project x11 to have the same size as x13\n",
        "        x13 = x13 + xS6\n",
        "\n",
        "        # BLOCK 4 #\n",
        "        x14 = F.relu(self.norm14(self.conv14(x13)))     # x14 has size 512\n",
        "        x15 = F.relu(self.norm15(self.conv15(x14)))     # x15 has size 512\n",
        "        xS7 = F.relu(self.normS7(self.shortcut7(x13)))  # have to project x13 to have the same size as x15\n",
        "        x15 = x15 + xS7\n",
        "        x16 = F.relu(self.norm16(self.conv16(x15)))     # x16 has size 512\n",
        "        x17 = F.relu(self.norm17(self.conv17(x16)))     # x17 has size 512\n",
        "        xS8 = F.relu(self.normS8(self.shortcut8(x15)))  # have to project x15 to have the same size as x17\n",
        "        x17 = x17 + xS8\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ePr-lv5cn2Sn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''train network 1'''\n",
        "\n",
        "device = 'cuda'\n",
        "best_acc = 0  # best test accuracy\n",
        "start_epoch = 0  # start from epoch 0 or last checkpoint epoch\n",
        "num_epochs = 210\n",
        "num_layers = 17\n",
        "\n",
        "absolute_layer_energies1 = np.zeros((num_epochs, num_layers+1))\n",
        "fractional_layer_energies1 = np.zeros((num_epochs, num_layers+1))\n",
        "\n",
        "\n",
        "print('==> Building model..')\n",
        "net1 = Net1()\n",
        "net1 = net1.to(device)\n",
        "net1 = torch.nn.DataParallel(net1)\n",
        "cudnn.benchmark = True\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(net1.parameters(), lr=0.08, momentum=0.9, weight_decay=5e-4)\n",
        "\n",
        "# Training\n",
        "def train(epoch):\n",
        "    print('\\nEpoch: %d' % epoch)\n",
        "    net1.train()\n",
        "    train_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    activations = []\n",
        "    # global absolute_layer_energies\n",
        "    # global fractional_layer_energies\n",
        "    previous_time = time.process_time()\n",
        "    \n",
        "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs, activations = net1(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item()\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += targets.size(0)\n",
        "        correct += predicted.eq(targets).sum().item()\n",
        "        acc = correct/total    \n",
        "        if batch_idx%100==0:\n",
        "            current_time = time.process_time()\n",
        "            print(batch_idx, len(testloader), 'Lap time (s): %.2f | Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
        "                    % (current_time - previous_time, train_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
        "            previous_time = current_time\n",
        "            \n",
        "    this_epoch_abs_energies, this_epoch_frac_energies = count_non_zeros(activations)\n",
        "    fractional_layer_energies1[epoch] = this_epoch_frac_energies\n",
        "    absolute_layer_energies1[epoch] = this_epoch_abs_energies\n",
        "    print('Total activation density: %.3f' % (fractional_layer_energies1[epoch, 0]))\n",
        "\n",
        "\n",
        "def test(epoch):\n",
        "    global best_acc\n",
        "    net1.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    activations = []\n",
        "    previous_time = time.process_time()\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs, activations = net1(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            test_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += targets.size(0)\n",
        "            correct += predicted.eq(targets).sum().item()\n",
        "            acc = correct/total\n",
        "            if batch_idx%100 == 0:\n",
        "                best_acc = max(acc, best_acc)\n",
        "                current_time = time.process_time()\n",
        "                print(batch_idx, len(testloader), 'Lap time (s): %.2f | Loss: %.3f | Acc: %.3f%% (%d/%d) | Best acc: %.3f'\n",
        "                    % (current_time - previous_time, test_loss/(batch_idx+1), 100.*correct/total, correct, total, best_acc))\n",
        "                previous_time = current_time\n",
        "            if acc>=best_acc:\n",
        "                #print('Saving')\n",
        "                torch.save(net.state_dict(),'./sample_data/resnet18_net1.pth')\n",
        "\n",
        "\n",
        "def count_non_zeros(activations): \n",
        "    \n",
        "    #returns: numpy array containing the number of non-zero activations per layer (15x1)\n",
        "    #         numpy array containing the fraction of non-zero activations per layer (15x1)\n",
        "    \n",
        "    n = 0\n",
        "    num_zeros = np.zeros((num_layers+1,), dtype = int)\n",
        "    num_non_zeros = np.zeros((num_layers+1,), dtype = int)\n",
        "    total_activations = np.zeros((num_layers+1,), dtype = int)\n",
        "    fraction_non_zero = np.zeros((num_layers+1,), dtype = float)\n",
        "    for x in activations:\n",
        "        n += 1\n",
        "        #reshape activations into a flat list\n",
        "        num_activations = x.size()[0] * x.size()[1] * x.size()[2] * x.size()[3]\n",
        "        \n",
        "        y = x.view(num_activations).tolist()\n",
        "        \n",
        "        #count how many entries are zero / non-zero\n",
        "        num_zeros[n] = y.count(0)\n",
        "        total_activations[n] = num_activations\n",
        "        num_non_zeros[n] = len(y) - num_zeros[n]\n",
        "        fraction_non_zero[n] = num_non_zeros[n].astype(float)/float(len(y))\n",
        "\n",
        "        \n",
        "    #store total values in the zero slot\n",
        "    num_non_zeros[0] = np.sum(num_non_zeros) \n",
        "    total_activations[0] = np.sum(total_activations)\n",
        "    fraction_non_zero[0] = num_non_zeros[0].astype(float)/total_activations[0].astype(float)\n",
        "    return num_non_zeros, fraction_non_zero\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    if (epoch == 70):\n",
        "        optimizer = optim.SGD(net1.parameters(), lr=0.005, momentum=0.9, weight_decay=5e-4)\n",
        "    if (epoch == 140):\n",
        "        optimizer = optim.SGD(net1.parameters(), lr=0.0005, momentum=0.9, weight_decay=5e-4)\n",
        "    train(epoch)\n",
        "    test(epoch)\n",
        "    print('Elapsed time: %.2f' % (time.process_time()))\n",
        "\n",
        "\n",
        "torch.save(fractional_layer_energies1,'./sample_data/fraction_energy_net1.pth')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sNL48y67oZUG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "total_energy = fractional_layer_energies[:,0]\n",
        "conv1_energy = fractional_layer_energies[:,1]\n",
        "conv2_energy = fractional_layer_energies[:,2]\n",
        "conv3_energy = fractional_layer_energies[:,3]\n",
        "conv4_energy = fractional_layer_energies[:,4]\n",
        "conv5_energy = fractional_layer_energies[:,5]\n",
        "conv6_energy = fractional_layer_energies[:,6]\n",
        "conv7_energy = fractional_layer_energies[:,7]\n",
        "conv8_energy = fractional_layer_energies[:,8]\n",
        "conv9_energy = fractional_layer_energies[:,9]\n",
        "conv10_energy = fractional_layer_energies[:,10]\n",
        "conv11_energy = fractional_layer_energies[:,11]\n",
        "conv12_energy = fractional_layer_energies[:,12]\n",
        "conv13_energy = fractional_layer_energies[:,13]\n",
        "conv14_energy = fractional_layer_energies[:,14]\n",
        "conv15_energy = fractional_layer_energies[:,15]\n",
        "conv16_energy = fractional_layer_energies[:,16]\n",
        "conv17_energy = fractional_layer_energies[:,17]\n",
        "\n",
        "\n",
        "\n",
        "ylim = 1\n",
        "ylow = 0 \n",
        "\n",
        "fig = plt.figure(1, figsize=(15, 7), dpi=95)\n",
        "#plt.subplot(231)\n",
        "#plt.ylim(0, 1)\n",
        "#plt.plot(test_acc, label = 'Test accuracy')\n",
        "#plt.legend(frameon = False)\n",
        "\n",
        "plt.subplot(232)\n",
        "plt.ylim(ylow, ylim)\n",
        "plt.plot(total_energy, label = 'Total energy')\n",
        "plt.legend(frameon = False)\n",
        "\n",
        "plt.subplot(233)\n",
        "plt.ylim(ylow, ylim)\n",
        "#plt.plot(conv1_energy, label = 'Conv1 energy')\n",
        "plt.plot(conv2_energy, label = 'Conv2 energy')\n",
        "plt.plot(conv3_energy, label = 'Conv3 energy')\n",
        "plt.plot(conv4_energy, label = 'Conv4 energy')\n",
        "plt.plot(conv5_energy, label = 'Conv5 energy')\n",
        "plt.legend(frameon = False)\n",
        "\n",
        "plt.subplot(234)\n",
        "plt.ylim(ylow, ylim)\n",
        "plt.plot(conv6_energy, label = 'Conv6 energy')\n",
        "plt.plot(conv7_energy, label = 'Conv7 energy')\n",
        "plt.plot(conv8_energy, label = 'Conv8 energy')\n",
        "plt.plot(conv9_energy, label = 'Conv9 energy')\n",
        "plt.legend(frameon = False)\n",
        "\n",
        "plt.subplot(235)\n",
        "plt.ylim(ylow, ylim)\n",
        "plt.plot(conv10_energy, label = 'Conv10 energy')\n",
        "plt.plot(conv11_energy, label = 'Conv11 energy')\n",
        "plt.plot(conv12_energy, label = 'Conv12 energy')\n",
        "plt.plot(conv13_energy, label = 'Conv13 energy')\n",
        "plt.legend(frameon = False)\n",
        "\n",
        "plt.subplot(236)\n",
        "plt.ylim(ylow, ylim)\n",
        "plt.plot(conv14_energy, label = 'Conv14 energy')\n",
        "plt.plot(conv15_energy, label = 'Conv15 energy')\n",
        "plt.plot(conv16_energy, label = 'Conv16 energy')\n",
        "plt.plot(conv17_energy, label = 'Conv17 energy')\n",
        "plt.legend(frameon = False)\n",
        "\n",
        "\n",
        "fig.suptitle('Fractional activation energy (sparsity) vs. epoch', fontsize = 16)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}